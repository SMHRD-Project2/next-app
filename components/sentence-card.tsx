"use client"

import { useEffect, useState, useRef } from "react"
import { Card, CardContent, CardHeader, CardTitle } from "@/components/ui/card"
import { Button } from "@/components/ui/button"
import { RefreshCw, Pencil, Volume2, ChevronDown, Play, Pause, Star, MessageSquare, Speech, Mic, Square, ArrowRight, Download, Lock, LogIn } from "lucide-react";
import {
  DropdownMenu,
  DropdownMenuContent,
  DropdownMenuItem,
  DropdownMenuTrigger,
  DropdownMenuSeparator,
} from "@/components/ui/dropdown-menu"
import { Avatar, AvatarImage, AvatarFallback } from "@/components/ui/avatar"
import { useAIModels } from "@/lib/ai-model-context"
import { LoadingMessage } from "@/components/loading-message"
import { Progress } from "@/components/ui/progress"
import {
  WaveformPlayer,
  type WaveformPlayerHandle,
} from "@/components/waveform-player"
import { getAuthStatus } from "@/lib/auth-utils"

interface SentenceCardProps {
  sentence: string
  voiceUrl1?: string  // DBì—ì„œ ê°€ì ¸ì˜¨ voiceUrl ì¶”ê°€
  voiceUrl2?: string  // DBì—ì„œ ê°€ì ¸ì˜¨ voiceUrl ì¶”ê°€
  voiceUrl3?: string  // DBì—ì„œ ê°€ì ¸ì˜¨ voiceUrl ì¶”ê°€
  onSentenceChange?: (newSentence: string) => void  // ë¬¸ì¥ ìˆ˜ì • ì½œë°±
  onRefresh?: () => void
  currentTab: string             // í˜„ì¬ íƒ­ ì •ë³´ë¥¼ propsë¡œ ë°›ê¸°
  isRecording: boolean
  onRecord: () => void
  hasRecorded: boolean
  onNext: () => void
  canNext: boolean
  waveformRef: React.RefObject<WaveformPlayerHandle>
  onRecordingComplete?: (url: string | null) => void
  onAnalysisComplete?: (analysisResult: any, referenceUrl?: string, userRecordingUrl?: string) => void  // ë¶„ì„ ê²°ê³¼ ì½œë°± ì¶”ê°€
}

export function SentenceCard({
  sentence,
  voiceUrl1,  // voiceUrl prop ì¶”ê°€
  voiceUrl2,  // voiceUrl prop ì¶”ê°€
  voiceUrl3,  // voiceUrl prop ì¶”ê°€
  onSentenceChange,
  onRefresh,
  currentTab,
  isRecording,
  onRecord,
  hasRecorded,
  onNext,
  canNext,
  waveformRef,
  onRecordingComplete,
  onAnalysisComplete
}: SentenceCardProps) {
  const { models: aiModels, isLoading, defaultModelId } = useAIModels()
  const [waveformHeights, setWaveformHeights] = useState<number[]>([])
  const [isClient, setIsClient] = useState(false)
  const [isLoggedIn, setIsLoggedIn] = useState(false)
  const [isAnalyzing, setIsAnalyzing] = useState(false)  // ë¶„ì„ ìƒíƒœ ì¶”ê°€
  const [isTTSLoading, setIsTTSLoading] = useState(false)  // TTS ë¡œë”© ìƒíƒœ ì¶”ê°€
  const [hasAnalyzed, setHasAnalyzed] = useState(false)  // ë¶„ì„ ì™„ë£Œ ìƒíƒœ ì¶”ê°€

  // TTS ìºì‹œ ìƒíƒœ ì¶”ê°€
  const [ttsCache, setTtsCache] = useState<Map<string, string>>(new Map())

  // TTS progress state (for AI announcer playback)
  const [ttsProgress, setTtsProgress] = useState<number | null>(null)
  const eventSourceRef = useRef<EventSource | null>(null)
  const pendingAudioRef = useRef<HTMLAudioElement | null>(null)

  const maybePlayPendingAudio = () => {
    if (ttsProgress !== null && ttsProgress >= 100 && pendingAudioRef.current) {
      const audio = pendingAudioRef.current
      aiExampleAudioRef.current = audio
      audio.play()
      pendingAudioRef.current = null
    }
  }

  const MAX_LENGTH = 300;

  // 250609 ë°•ë‚¨ê·œ - ë‚´ë¶€ ë¬¸ì¥ ìƒíƒœë¥¼ ë”°ë¡œ ê´€ë¦¬í•˜ë„ë¡ ìˆ˜ì •
  const [localSentence, setLocalSentence] = useState(sentence);
  
  // console.log("Initial sentence prop:", sentence);  // ì´ˆê¸° sentence prop ê°’ í™•ì¸

  // textarea ì°¸ì¡°ë¥¼ ìœ„í•œ ref ì¶”ê°€
  const textareaRef = useRef<HTMLTextAreaElement>(null);

  // localSentenceê°€ ë³€ê²½ë  ë•Œë§ˆë‹¤ ë†’ì´ ì¡°ì ˆ
  useEffect(() => {
    if (textareaRef.current) {
      textareaRef.current.style.height = '0px';
      textareaRef.current.style.height = textareaRef.current.scrollHeight + 'px';
    }
  }, [localSentence]);

  // 250609 ë°•ë‚¨ê·œ - ë¶€ëª¨ props sentence ë³€ê²½ ì‹œ ë‚´ë¶€ ìƒíƒœ ë™ê¸°í™”
  useEffect(() => {
    // console.log("Sentence prop changed to:", sentence);  // sentence prop ë³€ê²½ ì‹œì  ë¡œê·¸
    setLocalSentence(sentence);
    // ë¬¸ì¥ì´ ë³€ê²½ë˜ë©´ ê¸°ì¡´ TTS ìºì‹œ ì´ˆê¸°í™”
    setTtsCache(new Map());
    // console.log("localSentence updated to:", sentence);  // localSentence ì—…ë°ì´íŠ¸ í›„ ê°’ í™•ì¸
  }, [sentence]);

  useEffect(() => {
    // í´ë¼ì´ì–¸íŠ¸ ë§ˆìš´íŠ¸ í™•ì¸
    setIsClient(true)
    // í´ë¼ì´ì–¸íŠ¸ ì‚¬ì´ë“œì—ì„œë§Œ ëœë¤ ê°’ ìƒì„±
    const heights = Array.from({ length: 40 }, () => Math.random() * 30 + 10)
    setWaveformHeights(heights)

    // í´ë¼ì´ì–¸íŠ¸ì—ì„œë§Œ ë¡œê·¸ì¸ ìƒíƒœ í™•ì¸
    if (typeof window !== "undefined") {
      const { isLoggedIn: loggedIn } = getAuthStatus()
      setIsLoggedIn(loggedIn)
    }

    // ë¡œê·¸ì¸ ìƒíƒœ ë³€ê²½ ê°ì§€
    const handleAuthChange = () => {
      if (typeof window !== "undefined") {
        const { isLoggedIn: loggedIn } = getAuthStatus()
        setIsLoggedIn(loggedIn)
      }
    }

    window.addEventListener('localStorageChange', handleAuthChange)
    return () => window.removeEventListener('localStorageChange', handleAuthChange)
  }, [])

  // // Cleanup SSE when unmounting
  // useEffect(() => {
  //   return () => {
  //     if (eventSourceRef.current) {
  //       eventSourceRef.current.close()
  //     }
  //   }
  // }, [])

  // 250609 ë°•ë‚¨ê·œ - textarea onChange í•¸ë“¤ëŸ¬ì—ì„œ ë‚´ë¶€ ìƒíƒœ ë° ë¶€ëª¨ ì½œë°± í˜¸ì¶œ
  const handleSentenceChange = (e: React.ChangeEvent<HTMLTextAreaElement>) => {
    const inputText = e.target.value;
    // console.log("Textarea input:", inputText);  // ì…ë ¥ëœ í…ìŠ¤íŠ¸ í™•ì¸
    const truncatedText = inputText.slice(0, MAX_LENGTH);
    setLocalSentence(truncatedText);
    // ë¬¸ì¥ì´ ë³€ê²½ë˜ë©´ ê¸°ì¡´ TTS ìºì‹œ ì´ˆê¸°í™”
    setTtsCache(new Map());
    // console.log("localSentence set to:", truncatedText);  // localSentence ì„¤ì • ê°’ í™•ì¸
    
    if (onSentenceChange) onSentenceChange(truncatedText);
  }

  // selectedModel ìƒíƒœ ì´ˆê¸°í™” ë¡œì§ ê°œì„  ë° aiModels ë³€ê²½ ì‹œ ì—…ë°ì´íŠ¸
  const [selectedModel, setSelectedModel] = useState<number | null>(() => {
    // ì´ˆê¸°ê°’ì„ localStorageì—ì„œ ê°€ì ¸ì˜´
    if (typeof window !== 'undefined') {
      const savedDefaultId = localStorage.getItem('defaultModelId');
      return savedDefaultId ? parseInt(savedDefaultId) : null;
    }
    return null;
  });
  
  useEffect(() => {
    if (isLoading) return;
    
    // Find the default model and set it as selected
    const defaultModel = aiModels.find(model => model._id === defaultModelId);
    if (defaultModel) {
      setSelectedModel(defaultModel.id);
    }
    
    // Listen for model changes
    const handleModelChange = () => {
      const defaultModel = aiModels.find(model => model._id === defaultModelId);
      if (defaultModel) {
        setSelectedModel(defaultModel.id);
      }
    };
    
    window.addEventListener('aiModelChange', handleModelChange);
    return () => window.removeEventListener('aiModelChange', handleModelChange);
  }, [aiModels, isLoading, defaultModelId]);

  const [playingModel, setPlayingModel] = useState<number | null>(null)
  const selectedModelAudioRef = useRef<HTMLAudioElement | null>(null);
  const aiExampleAudioRef = useRef<HTMLAudioElement | null>(null);

  const [isPlayingSelectedModel, setIsPlayingSelectedModel] = useState(false);
  const [isPlayingAIExample, setIsPlayingAIExample] = useState(false);

  // const handlePlaySelectedModelSentence = async () => {
  //   // í˜„ì¬ ì¬ìƒ ì¤‘ì¸ AI ì˜ˆì‹œ ìŒì„± ì¤‘ì§€ (ë§Œì•½ ìˆë‹¤ë©´)
  //   if (aiExampleAudioRef.current) {
  //     aiExampleAudioRef.current.pause();
  //     URL.revokeObjectURL(aiExampleAudioRef.current.src);
  //     aiExampleAudioRef.current = null;
  //     setIsPlayingAIExample(false);
  //   }

  //   // ì´ë¯¸ ì„ íƒëœ ëª¨ë¸ì˜ ìŒì„±ì´ ì¬ìƒ ì¤‘ì´ë©´ ì¤‘ì§€í•˜ê³  ì´ˆê¸°í™”
  //   if (isPlayingSelectedModel) {
  //     if (selectedModelAudioRef.current) {
  //       selectedModelAudioRef.current.pause();
  //       URL.revokeObjectURL(selectedModelAudioRef.current.src);
  //       selectedModelAudioRef.current = null;
  //     }
  //     setIsPlayingSelectedModel(false);
  //     return;
  //   }

  //   // ì„ íƒëœ ëª¨ë¸ì´ ì—†ìœ¼ë©´ ê²½ê³  í›„ ì¢…ë£Œ (ì‹¤ì œ ëª¨ë¸ ì„ íƒì´ í•„ìš”í•œ ê²½ìš°)
  //   if (!selectedModel) {
  //     console.warn("ì„ íƒëœ ëª¨ë¸ì´ ì—†ì–´ ìŒì„±ì„ ì¬ìƒí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.");
  //     return;
  //   }

  //   try {
  //     setIsPlayingSelectedModel(true);

  //     // AI ëª¨ë¸ DBì—ì„œ URL ê°€ì ¸ì˜¤ê¸°
  //     const modelUrl = aiModels.find(model => model.id === selectedModel)?.url;
  //     if (!modelUrl) {
  //       throw new Error("ëª¨ë¸ URLì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.");
  //     }

  //     // ìŒì„± íŒŒì¼ ê°€ì ¸ì˜¤ê¸°
  //     const voiceResponse = await fetch(modelUrl);
  //     const voiceBlob = await voiceResponse.blob();
  //     const audioUrl = URL.createObjectURL(voiceBlob);
      
  //     const audio = new Audio(audioUrl);
  //     selectedModelAudioRef.current = audio;

  //     audio.onended = () => {
  //       setIsPlayingSelectedModel(false);
  //       URL.revokeObjectURL(audioUrl);
  //       if (selectedModelAudioRef.current === audio) {
  //         selectedModelAudioRef.current = null;
  //       }
  //     };

  //     audio.play();

  //   } catch (error) {
  //     console.error('ìŒì„± ì¬ìƒ ì¤‘ ì˜¤ë¥˜:', error);
  //     setIsPlayingSelectedModel(false);
  //     selectedModelAudioRef.current = null;
  //   }
  // };

  const handlePlayAIExample = async () => {
    if (!selectedModel) {
      console.error("ì„ íƒëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.");
      return;
    }

    // ì´ë¯¸ TTS ë¡œë”© ì¤‘ì´ë©´ ì·¨ì†Œ
    if (isTTSLoading) {
      console.log("TTSê°€ ì´ë¯¸ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤.");
      return;
    }

    // í˜„ì¬ ì¬ìƒ ì¤‘ì¸ ì˜¤ë””ì˜¤ê°€ ìˆë‹¤ë©´ ì¤‘ì§€
    if (currentAudioRef.current) {
      currentAudioRef.current.pause();
      currentAudioRef.current = null;
      setIsPlayingAIExample(false);
      return;
    }

    try {
      // ìºì‹œ í‚¤ ìƒì„± (ë¬¸ì¥ + ëª¨ë¸ ID + íƒ­)
      const cacheKey = `${localSentence}_${selectedModel}_${currentTab}`;
      
      // ìºì‹œëœ TTS ê²°ê³¼ê°€ ìˆëŠ”ì§€ í™•ì¸
      let audioUrl: string | null = ttsCache.get(cacheKey) || null;
      
      if (audioUrl) {
        console.log("ìºì‹œëœ TTS ê²°ê³¼ ì‚¬ìš©:", cacheKey);
        
        // ìºì‹œëœ URLë¡œ ë°”ë¡œ ì¬ìƒ
        const audio = new Audio(audioUrl);
        currentAudioRef.current = audio;
        
        audio.onended = () => {
          setIsPlayingAIExample(false);
          currentAudioRef.current = null;
        };

        audio.onerror = () => {
          setIsPlayingAIExample(false);
          currentAudioRef.current = null;
        };

        await audio.play();
        setIsPlayingAIExample(true);
        return;
      }

      setIsTTSLoading(true);  // TTS ë¡œë”© ì‹œì‘
      console.log("ìƒˆë¡œìš´ TTS ìƒì„± ì‹œì‘:", cacheKey);
      
      // DBì˜ voiceUrlì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ TTS ìƒì„±
      audioUrl = null;
      
      if (currentTab !== 'custom') {
        const modelDetails = aiModels.find(model => model.id === selectedModel);
        if (modelDetails?.name === 'ê¹€ì£¼í•˜ ì•„ë‚˜ìš´ì„œ' && voiceUrl1) {
          audioUrl = voiceUrl1;
        } else if (modelDetails?.name === 'ì´ë™ìš± ì•„ë‚˜ìš´ì„œ' && voiceUrl2) {
          audioUrl = voiceUrl2;
        } else if (modelDetails?.name === 'ë°•ì†Œí˜„ ì•„ë‚˜ìš´ì„œ' && voiceUrl3) {
          audioUrl = voiceUrl3;
        }
      }

      // DBì— ìŒì„±ì´ ì—†ê±°ë‚˜ custom íƒ­ì¸ ê²½ìš° TTS ì‚¬ìš©
      if (!audioUrl) {
        console.log("Using TTS for playback");
        const modelUrl = aiModels.find(model => model.id === selectedModel)?.url;
        console.log("Selected Model URL:", modelUrl);
        console.log("Selected localSentence:", localSentence);

        // ìŒì„± íŒŒì¼ ê°€ì ¸ì˜¤ê¸°
        const voiceResponse = await fetch(modelUrl || '');
        const voiceBlob = await voiceResponse.blob();

        // ë¬´ìŒ íŒŒì¼ ê°€ì ¸ì˜¤ê¸°
        const silenceResponse = await fetch('/audio/silence_100ms.wav');
        const silenceBlob = await silenceResponse.blob();

        // FormData ìƒì„±
        const formData = new FormData();
        formData.append('voice_file', voiceBlob, modelUrl?.split('/').pop() || '');
        formData.append('silence_file', silenceBlob, 'silence_100ms.wav');

        console.log('ì „ì†¡í•  ë°ì´í„°:', {
          text: localSentence,
          voiceFileName: modelUrl?.split('/').pop(),
          formDataKeys: Array.from(formData.keys())
        });

        // Next.js APIë¥¼ í†µí•´ ìš”ì²­
        const response = await fetch(`/api/tts?text=${encodeURIComponent(localSentence)}`, {
          method: 'POST',
          body: formData,
        });

        if (!response.ok) {
          const errorText = await response.text();
          throw new Error(`TTS ë³€í™˜ ì‹¤íŒ¨: ${errorText}`);
        }

        // TTS APIëŠ” JSON ì‘ë‹µìœ¼ë¡œ S3 URLì„ ë°˜í™˜
        const jsonResponse = await response.json();
        console.log("TTS JSON ì‘ë‹µ:", jsonResponse);
        
        if (jsonResponse.success && jsonResponse.url) {
          // S3 URLì„ ì§ì ‘ ì‚¬ìš©
          audioUrl = jsonResponse.url;
          console.log("TTS ê²°ê³¼ S3 URL:", audioUrl);
        } else {
          throw new Error("TTS ì‘ë‹µì— ìœ íš¨í•œ URLì´ ì—†ìŠµë‹ˆë‹¤.");
        }
      }

      // TTS ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥
      if (audioUrl) {
        setTtsCache(prev => {
          const newCache = new Map(prev);
          newCache.set(cacheKey, audioUrl!);
          console.log("TTS ê²°ê³¼ ìºì‹œ ì €ì¥:", cacheKey);
          return newCache;
        });
      }

      // ìƒˆë¡œìš´ ì˜¤ë””ì˜¤ ìƒì„± ë° ì¬ìƒ
      const audio = new Audio(audioUrl!);
      currentAudioRef.current = audio;
      
      audio.onended = () => {
        setIsPlayingAIExample(false);
        currentAudioRef.current = null;
      };

      audio.onerror = () => {
        setIsPlayingAIExample(false);
        currentAudioRef.current = null;
      };

      await audio.play();
      setIsPlayingAIExample(true);
    } catch (error) {
      console.error('TTS ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜:', error);
      setIsPlayingAIExample(false);
      currentAudioRef.current = null;
    } finally {
      setIsTTSLoading(false);  // TTS ë¡œë”© ì™„ë£Œ
    }
  };

  // const handleTTSPlayback = async (modelDetails: typeof aiModels[0]) => {
  //   try {
  //     console.log("Starting TTS conversion for model:", modelDetails.name);
  //     console.log("Text to convert:", localSentence);
      
  //     const response = await fetch("/api/tts", {
  //       method: "POST",
  //       headers: {
  //         "Content-Type": "application/json",
  //       },
  //       body: JSON.stringify({
  //         text: localSentence,
  //         modelId: modelDetails.id,
  //       }),
  //     });

  //     if (!response.ok) {
  //       const errorText = await response.text();
  //       console.error("TTS API Error Response:", errorText);
  //       throw new Error(`TTS conversion failed: ${errorText}`);
  //     }

  //     const data = await response.json();
  //     console.log("TTS API Response:", data);
      
  //     if (!data.audioUrl) {
  //       throw new Error("No audio URL in TTS response");
  //     }

  //     // í˜„ì¬ ì¬ìƒ ì¤‘ì¸ ì˜¤ë””ì˜¤ê°€ ìˆë‹¤ë©´ ì¤‘ì§€
  //     if (currentAudioRef.current) {
  //       currentAudioRef.current.pause();
  //       currentAudioRef.current = null;
  //     }

  //     // ìƒˆë¡œìš´ ì˜¤ë””ì˜¤ ìƒì„± ë° ì¬ìƒ
  //     const audio = new Audio(data.audioUrl);
  //     currentAudioRef.current = audio;
      
  //     audio.onended = () => {
  //       setIsPlaying(false);
  //       currentAudioRef.current = null;
  //     };

  //     await audio.play();
  //     setIsPlaying(true);
  //   } catch (error) {
  //     console.error("TTS Error:", error);
  //     setIsPlaying(false);
  //     currentAudioRef.current = null;
      
  //     // ì—ëŸ¬ ë°œìƒ ì‹œ ì‚¬ìš©ìì—ê²Œ ì•Œë¦¼
  //     alert("ìŒì„± ë³€í™˜ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.");
  //   }
  // };

  // ë…¹ìŒ ê´€ë ¨ ìƒíƒœë“¤ ì¶”ê°€
  const [audioURL, setAudioURL] = useState<string | null>(null)
  const [isPlaying, setIsPlaying] = useState(false)
  const [waveHeights, setWaveHeights] = useState<number[]>([])
  const mediaRecorderRef = useRef<MediaRecorder | null>(null)
  const audioChunksRef = useRef<Blob[]>([])  
  const [recordingTime, setRecordingTime] = useState(0)
  const timerRef = useRef<NodeJS.Timeout | null>(null)
  const [uploadedRecordingUrl, setUploadedRecordingUrl] = useState<string | null>(null)  // ì—…ë¡œë“œëœ ë…¹ìŒ URL ì €ì¥

  const currentAudioRef = useRef<HTMLAudioElement | null>(null);
  const audioRef = useRef<HTMLAudioElement | null>(null);  // ë…¹ìŒëœ ì˜¤ë””ì˜¤ ì¬ìƒìš© ref ì¶”ê°€

  // í‰ê°€í•˜ê¸° ë²„íŠ¼ í´ë¦­ í•¸ë“¤ëŸ¬ ì¶”ê°€
  const handleEvaluate = async () => {
    if (!uploadedRecordingUrl) {
      console.error("ì—…ë¡œë“œëœ ë…¹ìŒ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.");
      return;
    }
    
    await performVoiceAnalysis(uploadedRecordingUrl);
  }

  //  2506011 ë°•ë‚¨ê·œ aws ì—…ë¡œë“œí•˜ê¸°
  const uploadToS3 = async (blob: Blob, skipAnalysis: boolean = false) => {
    console.log("ì „ë‹¬ëœ blob:", blob)
    console.log("Blob íƒ€ì…:", blob.type)
    console.log("Blob í¬ê¸°:", blob.size)

    // blob íƒ€ì…ì— ë”°ë¼ íŒŒì¼ëª…ê³¼ íƒ€ì… ê²°ì •
    let fileName: string
    let fileType: string
    
    if (blob.type === "audio/wav" || blob.type === "audio/wave") {
      fileName = "recording.wav"
      fileType = "audio/wav"
    } else if (blob.type === "audio/webm") {
      fileName = "recording.webm"
      fileType = "audio/webm"
    } else {
      // ê¸°ë³¸ê°’ìœ¼ë¡œ webm ì‚¬ìš© (ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€)
      fileName = "recording.webm"
      fileType = "audio/webm"
      console.warn("ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë””ì˜¤ íƒ€ì…, ê¸°ë³¸ê°’(webm) ì‚¬ìš©:", blob.type)
    }

    const formData = new FormData()
    const file = new File([blob], fileName, { type: fileType })

    console.log("ìƒì„±ëœ File ê°ì²´:", file)
    console.log("File íƒ€ì…:", file.type)
    console.log("File í¬ê¸°:", file.size)
    console.log("íŒŒì¼ëª…:", fileName)

    formData.append("file", file)

    for (let [key, value] of formData.entries()) {
      console.log("FormData í•­ëª©:", key, value)
    }

    try {
      const res = await fetch(`${process.env.NEXT_PUBLIC_PY_URL}/upload_record`, {
        method: "POST",
        body: formData,
      })
      console.log("ì‘ë‹µ ìƒíƒœ:", res.status)

      if (!res.ok) {
        const errorText = await res.text()
        console.error("[ERROR] ì„œë²„ ì‘ë‹µ ì˜¤ë¥˜:", errorText)
        throw new Error(`ì„œë²„ ì‘ë‹µ ì˜¤ë¥˜: ${res.status} ${errorText}`)
      }

      const data = await res.json()

      if (data.success) {
        console.log("ì—…ë¡œë“œ ì„±ê³µ:", data.url)
        setUploadedRecordingUrl(data.url)  // ì—…ë¡œë“œëœ URL ì €ì¥
        if (onRecordingComplete) onRecordingComplete(data.url)
        
        // skipAnalysisê°€ falseì¼ ë•Œë§Œ ìŒì„± ë¶„ì„ ì‹œì‘ (ë¬´í•œ ë£¨í”„ ë°©ì§€)
        if (!skipAnalysis) {
          await performVoiceAnalysis(data.url)
        }
        
        return data.url
      } else {
        const errMsg = typeof data.error === "string" ? data.error : "ì—…ë¡œë“œ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        console.error("ì—…ë¡œë“œ ì‹¤íŒ¨:", data.error)
        throw new Error(errMsg)
      }
    } catch (error) {
      console.error("[ERROR] ì—…ë¡œë“œ ì¤‘ ì˜ˆì™¸ ë°œìƒ:", error)
      throw error
    }
  }

  // ìŒì„± ë¶„ì„ í•¨ìˆ˜ ì¶”ê°€
  const performVoiceAnalysis = async (userRecordingUrl: string) => {
    try {
      setIsAnalyzing(true);  // ë¶„ì„ ì‹œì‘
      
      // AI ì•„ë‚˜ìš´ì„œ ëª¨ë¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
      const modelDetails = aiModels.find(model => model.id === selectedModel);
      if (!modelDetails) {
        console.error("ì„ íƒëœ AI ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.");
        setIsAnalyzing(false);  // ë¶„ì„ ì‹¤íŒ¨ ì‹œ ìƒíƒœ ì´ˆê¸°í™”
        return;
      }

      // ë ˆí¼ëŸ°ìŠ¤ ìŒì„± URL ê²°ì •
      let referenceUrl = null;
      
      // ì•„ë‚˜ìš´ì„œ ëª¨ë¸ì´ê³  voiceUrlì´ ìˆìœ¼ë©°, custom íƒ­ì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ DBì˜ voiceUrl ì‚¬ìš©
      if (currentTab !== 'custom') {
        if (modelDetails.name === 'ê¹€ì£¼í•˜ ì•„ë‚˜ìš´ì„œ' && voiceUrl1) {
          referenceUrl = voiceUrl1;
        } else if (modelDetails.name === 'ì´ë™ìš± ì•„ë‚˜ìš´ì„œ' && voiceUrl2) {
          referenceUrl = voiceUrl2;
        } else if (modelDetails.name === 'ë°•ì†Œí˜„ ì•„ë‚˜ìš´ì„œ' && voiceUrl3) {
          referenceUrl = voiceUrl3;
        }
      }

      // DBì— ìŒì„±ì´ ì—†ê±°ë‚˜ custom íƒ­ì¸ ê²½ìš° ëª¨ë¸ URL ì‚¬ìš©
      if (!referenceUrl) {
        console.log("DBì— ìŒì„±ì´ ì—†ê±°ë‚˜ custom íƒ­ì¸ ê²½ìš° ëª¨ë¸ URL ì‚¬ìš©");
        const modelUrl = aiModels.find(model => model.id === selectedModel)?.url;
        console.log("Selected Model URL:", modelUrl);
        console.log("Selected localSentence:", localSentence);

        // ìŒì„± íŒŒì¼ ê°€ì ¸ì˜¤ê¸°
        const voiceResponse = await fetch(modelUrl || '');
        const voiceBlob = await voiceResponse.blob();

        // ë¬´ìŒ íŒŒì¼ ê°€ì ¸ì˜¤ê¸°
        const silenceResponse = await fetch('/audio/silence_100ms.wav');
        const silenceBlob = await silenceResponse.blob();

        // FormData ìƒì„±
        const formData = new FormData();
        formData.append('voice_file', voiceBlob, modelUrl?.split('/').pop() || '');
        formData.append('silence_file', silenceBlob, 'silence_100ms.wav');

        console.log('ì „ì†¡í•  ë°ì´í„°:', {
          text: localSentence,
          voiceFileName: modelUrl?.split('/').pop(),
          formDataKeys: Array.from(formData.keys())
        });

        // Next.js APIë¥¼ í†µí•´ ìš”ì²­
        const response = await fetch(`/api/tts?text=${encodeURIComponent(localSentence)}`, {
          method: 'POST',
          body: formData,
        });

        if (!response.ok) {
          const errorText = await response.text();
          throw new Error(`TTS ë³€í™˜ ì‹¤íŒ¨: ${errorText}`);
        }

        // TTS APIëŠ” JSON ì‘ë‹µìœ¼ë¡œ S3 URLì„ ë°˜í™˜
        const jsonResponse = await response.json();
        console.log("TTS JSON ì‘ë‹µ:", jsonResponse);
        
        if (jsonResponse.success && jsonResponse.url) {
          // S3 URLì„ ì§ì ‘ ì‚¬ìš©
          referenceUrl = jsonResponse.url;
          console.log("TTS ê²°ê³¼ S3 URL:", referenceUrl);
        } else {
          throw new Error("TTS ì‘ë‹µì— ìœ íš¨í•œ URLì´ ì—†ìŠµë‹ˆë‹¤.");
        }
      }

      if (!referenceUrl) {
        console.error("ë ˆí¼ëŸ°ìŠ¤ ìŒì„± URLì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.");
        return;
      }

      console.log("ìŒì„± ë¶„ì„ ì‹œì‘", {
        referenceUrl,
        userRecordingUrl,
        selectedModel: modelDetails.name,
        currentTab
      });

      // ìŒì„± ë¶„ì„ API í˜¸ì¶œ
      const analysisResponse = await fetch(`${process.env.NEXT_PUBLIC_PY_URL}/analyze-voice`, {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify({
          reference_url: referenceUrl,
          user_url: userRecordingUrl
        })
      });

      console.log("ìŒì„± ë¶„ì„ API ì‘ë‹µ ìƒíƒœ:", analysisResponse.status);
      console.log("ìŒì„± ë¶„ì„ API ì‘ë‹µ í—¤ë”:", Object.fromEntries(analysisResponse.headers.entries()));

      if (!analysisResponse.ok) {
        const errorText = await analysisResponse.text();
        console.error("ìŒì„± ë¶„ì„ API ì˜¤ë¥˜ ì‘ë‹µ:", errorText);
        throw new Error(`ìŒì„± ë¶„ì„ API í˜¸ì¶œ ì‹¤íŒ¨: ${analysisResponse.status} - ${errorText}`);
      }

      const analysisResult = await analysisResponse.json();
      
      console.log("ğŸ¯ ìŒì„± ë¶„ì„ ê²°ê³¼:", analysisResult);
      
      if (analysisResult.success) {
        console.log("ğŸ“Š ìƒì„¸ ë¶„ì„ ì ìˆ˜:");
        console.log("- ë°œìŒíŠ¹ì„±(MFCC) ì ìˆ˜:", analysisResult.analysis_result.mfcc);
        console.log("- ìŒì •(Pitch) ì ìˆ˜:", analysisResult.analysis_result.pitch);
        console.log("- ìŒëŸ‰(Energy) ì ìˆ˜:", analysisResult.analysis_result.energy);
        console.log("- ë°œìŒì†ë„(Speech-rate) ì ìˆ˜:", analysisResult.analysis_result.speed);
        console.log("- ìŒìƒ‰(Formant) ì ìˆ˜:", analysisResult.analysis_result.formant);
        console.log("- ìŒì •(Intonation) ì ìˆ˜:", analysisResult.analysis_result.intonation);
        console.log("- ë¦¬ë“¬(Rhythm) ì ìˆ˜:", analysisResult.analysis_result.rhythm);
        console.log("- ë¬¸ì¥ê°„ ì‰¼(Pause) ì ìˆ˜:", analysisResult.analysis_result.pause);
        console.log("ğŸ† ì¢…í•© ì ìˆ˜:", analysisResult.analysis_result.total);
        
        // OpenAI í”¼ë“œë°± ê²°ê³¼ ì¶œë ¥
        if (analysisResult.ai_feedback) {
          console.log("\nğŸ¤– OpenAI í”¼ë“œë°± ê²°ê³¼:");
          console.log("- ë¶„ì„ ID:", analysisResult.ai_feedback.analysisId);
          console.log("- ì „ì²´ ì ìˆ˜:", analysisResult.ai_feedback.overallScore);
          console.log("- í•­ëª©ë³„ í”¼ë“œë°±:");
          
          analysisResult.ai_feedback.items.forEach((item: any) => {
            console.log(`   ${item.metric} (${item.score}ì ):`);
            console.log(`     ì§§ì€ í”¼ë“œë°±: ${item.shortFeedback}`);
            console.log(`     ìƒì„¸ í”¼ë“œë°±:`, item.detailedFeedback);
          });
          
          // ì „ì²´ í”¼ë“œë°± ê°ì²´ë„ ì¶œë ¥ (ê°œë°œììš©)
          console.log("ğŸ” ì „ì²´ AI í”¼ë“œë°± ê°ì²´:", analysisResult.ai_feedback);
          
          // ë¶„ì„ ì™„ë£Œ ìƒíƒœë¥¼ trueë¡œ ì„¤ì •
          setHasAnalyzed(true);
          
          // ë¶„ì„ ê²°ê³¼ë¥¼ ë¶€ëª¨ ì»´í¬ë„ŒíŠ¸ë¡œ ì „ë‹¬
          if (onAnalysisComplete) {
            onAnalysisComplete(analysisResult.ai_feedback, referenceUrl, userRecordingUrl);
          }
        } else {
          console.log("âš ï¸ OpenAI í”¼ë“œë°±ì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.");
        }
      } else {
        console.error("ìŒì„± ë¶„ì„ ì‹¤íŒ¨:", analysisResult.error);
        console.error("ì „ì²´ ë¶„ì„ ê²°ê³¼:", analysisResult);
      }

    } catch (error) {
      console.error("ìŒì„± ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ:", error);
      console.error("ì—ëŸ¬ ìƒì„¸ ì •ë³´:", {
        message: error instanceof Error ? error.message : 'Unknown error',
        stack: error instanceof Error ? error.stack : undefined,
        error: error
      });
    } finally {
      setIsAnalyzing(false);  // ë¶„ì„ ì™„ë£Œ (ì„±ê³µ/ì‹¤íŒ¨ ê´€ê³„ì—†ì´)
    }
  }


  // ë…¹ìŒ ê´€ë ¨ í•¨ìˆ˜ë“¤ ì¶”ê°€
  const handleRecord = async () => {
    // console.log("handleRecord called. isRecording:", isRecording);
    if (!isRecording) {
      try {
        // console.log("Attempting to get media stream...");
        let stream;

        try {
          // ë¨¼ì € ì‹¤ì œ ë§ˆì´í¬ë¡œ ì‹œë„
          stream = await navigator.mediaDevices.getUserMedia({ audio: true });
        } catch (err) {
          // console.log("ë§ˆì´í¬ ì ‘ê·¼ ì‹¤íŒ¨, ê°€ìƒ ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ìƒì„± ì‹œë„");
          // ë§ˆì´í¬ ì ‘ê·¼ ì‹¤íŒ¨ ì‹œ ê°€ìƒ ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ìƒì„±
          const audioContext = new AudioContext();
          const oscillator = audioContext.createOscillator();
          const destination = audioContext.createMediaStreamDestination();
          oscillator.connect(destination);
          oscillator.start();
          stream = destination.stream;
        }


        let mimeType = 'audio/webm;codecs=opus'
        if (!MediaRecorder.isTypeSupported(mimeType)) {
          mimeType = 'audio/webm'
          if (!MediaRecorder.isTypeSupported(mimeType)) {
            mimeType = 'audio/mp4'
            if (!MediaRecorder.isTypeSupported(mimeType)) {
              mimeType = ''
            }
          }
        }

        const mediaRecorder = new MediaRecorder(stream, mimeType ? { mimeType } : undefined)

        mediaRecorderRef.current = mediaRecorder
        audioChunksRef.current = []

        mediaRecorder.ondataavailable = (event) => {
          if (event.data.size > 0) {
            audioChunksRef.current.push(event.data)
          }
        }

        mediaRecorder.onstop = () => {
          const audioBlob = new Blob(audioChunksRef.current, {
            type: mimeType || 'audio/webm'
          })

          if (audioURL) {
            URL.revokeObjectURL(audioURL)
          }

          const url = URL.createObjectURL(audioBlob)
          setAudioURL(url)
          if (onRecordingComplete) onRecordingComplete(url)
          audioChunksRef.current = []

          // ìƒˆë¡œìš´ ë…¹ìŒ ì‹œ ë¶„ì„ ì™„ë£Œ ìƒíƒœ ì´ˆê¸°í™”
          setHasAnalyzed(false)

          // 250611 ë°•ë‚¨ê·œ ì—…ë¡œë“œ - skipAnalysis: trueë¡œ ìë™ ë¶„ì„ ë¹„í™œì„±í™”
          uploadToS3(audioBlob, true)
          audioChunksRef.current = []


          if (timerRef.current) {
            clearInterval(timerRef.current)
            timerRef.current = null
          }
          setRecordingTime(0)

        }
        if (onRecordingComplete) onRecordingComplete(null)
        mediaRecorder.start()
        onRecord()

        setRecordingTime(0)
        timerRef.current = setInterval(() => {
          setRecordingTime(prev => prev + 1)
        }, 1000)
      } catch (err) {
        console.error('ë…¹ìŒ ê¶Œí•œì„ ì–»ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤:', err)
        alert('ë§ˆì´í¬ ì ‘ê·¼ ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤. ë¸Œë¼ìš°ì € ì„¤ì •ì—ì„œ ë§ˆì´í¬ ê¶Œí•œì„ í™•ì¸í•´ì£¼ì„¸ìš”.')
        // ì—¬ê¸°ì—ì„œ err ê°ì²´ë¥¼ ìì„¸íˆ ë¡œê¹…í•˜ì—¬ ì–´ë–¤ ì¢…ë¥˜ì˜ ì˜¤ë¥˜ì¸ì§€ í™•ì¸
        console.error("Error details:", err)
      }
    } else {
      if (mediaRecorderRef.current && mediaRecorderRef.current.state !== 'inactive') {
        mediaRecorderRef.current.stop()
        mediaRecorderRef.current.stream.getTracks().forEach(track => track.stop())
      }
      onRecord()
    }
  }

  // const handlePlay = async () => {
  //   if (audioRef.current && audioURL) {
  //     try {
  //       if (isPlaying) {
  //         audioRef.current.pause()
  //         setIsPlaying(false)
  //       } else {
  //         audioRef.current.src = audioURL
  //         await audioRef.current.play()
  //         setIsPlaying(true)
  //       }
  //     } catch (err) {
  //       console.error('ì¬ìƒ ì˜¤ë¥˜:', err)
  //       setIsPlaying(false)
  //     }
  //   }
  // }
  const handlePlay = async () => {
    if (!audioURL) return;
    
    try {
      if (isPlaying) {
        if (audioRef.current) {
          audioRef.current.pause();
        }
        setIsPlaying(false);
      } else {
        if (audioRef.current) {
          audioRef.current.src = audioURL;
          await audioRef.current.play();
          setIsPlaying(true);
        }
      }
    } catch (err) {
      console.error('ì¬ìƒ ì˜¤ë¥˜:', err);
      setIsPlaying(false);
    }
  };

  const handleDownload = () => {
    if (audioURL) {
      const link = document.createElement('a')
      link.href = audioURL
      link.download = 'recorded-audio.wav'
      document.body.appendChild(link)
      link.click()
      document.body.removeChild(link)
    }
  }

  const handleLoginRedirect = () => {
    window.location.href = "/auth/login"
  }

  // const handleWordClick = (index: number) => {
  //   const totalDuration = waveformRef.current?.getDuration()
  //   if (!totalDuration || words.length === 0) return

  //   const timePerWord = totalDuration / words.length
  //   waveformRef.current?.setCurrentTime(timePerWord * index)
  //   waveformRef.current?.play()

  //   setIsPlaying(true)
  //   setHighlightIndex(index)
  // }

  return (
    <Card className="bg-onair-bg-sub border-onair-text-sub/20">
      <CardHeader>
        <CardTitle className="text-onair-text flex items-center justify-between">
          <span>í›ˆë ¨ ë¬¸ì¥</span>
          <div className="flex items-center space-x-2 ml-auto">
            {onRefresh && (
              <Button
                variant="outline"
                size="sm"
                className="border-onair-mint text-onair-mint hover:bg-onair-mint hover:text-onair-bg"
                onClick={onRefresh}
                aria-label="ë¬¸ì¥ ìƒˆë¡œê³ ì¹¨"
                title="ë‹¤ë¥¸ ë¬¸ì¥ìœ¼ë¡œ êµì²´"
              >
                <RefreshCw className="w-4 h-4" />
              </Button>
            )}

            <div className="inline-flex rounded-md shadow-sm border border-onair-mint relative">
              <Button
                variant="ghost"
                size="sm"
                className="relative inline-flex items-center rounded-l-md rounded-r-none border-r border-onair-mint text-onair-mint hover:bg-onair-mint hover:text-onair-bg focus:z-10 focus:outline-none focus:ring-1 focus:ring-onair-mint"
                onClick={handlePlayAIExample}
                disabled={isTTSLoading}
              >
                {isTTSLoading ? (
                  <>
                    <RefreshCw className="w-4 h-4 mr-2 animate-spin" />
                    ìŒì„± ìƒì„± ì¤‘...
                  </>
                ) : isPlayingAIExample ? (
                  <>
                    <Pause className="w-4 h-4 mr-2" />
                    ì¬ìƒ ì¤‘ì§€
                  </>
                ) : (
                  <>
                    <Volume2 className="w-4 h-4 mr-2" />
                    {selectedModel ? aiModels.find(model => model.id === selectedModel)?.name : 'AI ì˜ˆì‹œ ë“£ê¸°'}
                  </>
                )}
              </Button>
              <DropdownMenu>
                <DropdownMenuTrigger asChild>
                  <Button
                    variant="ghost"
                    size="sm"
                    className="relative inline-flex items-center rounded-r-md rounded-l-none text-onair-mint hover:bg-onair-mint hover:text-onair-bg px-2 focus:z-10 focus:outline-none focus:ring-1 focus:ring-onair-mint"
                    aria-label="AI ëª¨ë¸ ì„ íƒ"
                    disabled={isTTSLoading}
                  >
                    <ChevronDown className="w-4 h-4" />
                  </Button>
                </DropdownMenuTrigger>
                <DropdownMenuContent align="end" className="w-[200px]">
                  {aiModels.map((model) => (
                    <DropdownMenuItem
                      key={model.id}
                      onClick={() => {
                        setSelectedModel(model.id);
                        // Stop AI Example playback if it's playing when a new model is selected
                        if (aiExampleAudioRef.current) {
                          aiExampleAudioRef.current.pause();
                          URL.revokeObjectURL(aiExampleAudioRef.current.src);
                          aiExampleAudioRef.current = null;
                          setIsPlayingAIExample(false);
                        }
                      }}
                      className="flex items-center space-x-2 cursor-pointer"
                    >
                      <Avatar className="w-6 h-6">
                        <AvatarImage src={model.avatar} />
                        <AvatarFallback className="bg-onair-bg text-onair-mint">
                          {model.name.charAt(0)}
                        </AvatarFallback>
                      </Avatar>
                      <div className="flex flex-col">
                        <span className="font-medium">{model.name}</span>
                        <span className="text-xs text-onair-text-sub">{model.type}</span>
                      </div>
                      {selectedModel === model.id && (
                        <span className="ml-auto text-onair-mint">âœ“</span>
                      )}
                      {model.id && selectedModel == model.id && (
                        <Star className="w-4 h-4 text-onair-orange fill-current ml-auto" />
                      )}
                    </DropdownMenuItem>
                  ))}
                </DropdownMenuContent>
              </DropdownMenu>

              {/* ë¹„íšŒì› ë¸”ëŸ¬ ì²˜ë¦¬ ì˜¤ë²„ë ˆì´ - AI ì˜ˆì‹œ ë“£ê¸° ë²„íŠ¼ë§Œ */}
              {!isLoggedIn && (
                <div className="absolute inset-0 bg-onair-bg/80 backdrop-blur-sm rounded-md flex items-center justify-center">
                    <Lock className="w-4 h-4 text-onair-mint mx-auto mb-1" />
                  {/* <div className="text-center p-2 bg-onair-bg-sub rounded border border-onair-text-sub/20">
                    <p className="text-xs text-onair-text-sub">ë¡œê·¸ì¸ í•„ìš”</p>
                  </div> */}
                </div>
              )}
            </div>
          </div>
        </CardTitle>
      </CardHeader>

      <CardContent>
        {/* 250609 ë°•ë‚¨ê·œ - í›ˆë ¨ ë¬¸ì¥ì„ ì§ì ‘ ë³´ì—¬ì£¼ê³  ìˆ˜ì • ê°€ëŠ¥í•˜ë„ë¡ ì²˜ë¦¬ */}
        <div className="p-6 bg-onair-bg rounded-lg border border-onair-text-sub/10">
          <textarea
            ref={textareaRef}
            className="w-full min-h-[8rem] resize-none bg-onair-bg text-onair-text text-lg leading-relaxed text-center rounded-md border border-onair-text-sub/20 p-4 [&::-webkit-scrollbar]:hidden [-ms-overflow-style:none] [scrollbar-width:none]"
            value={localSentence}
            onChange={handleSentenceChange}
            spellCheck={false}
            readOnly={currentTab !== 'custom' || isPlayingAIExample || isTTSLoading}
            maxLength={MAX_LENGTH}
          />
          <p className="text-sm text-onair-text-sub text-right">
            {localSentence.length}/300
          </p>
          {currentTab === 'custom' && ttsProgress !== null && (
            <div className="mt-2">
              <div className="flex justify-between text-xs mb-1">
                <span className="text-onair-text-sub">AI ì§„í–‰ë¥ </span>
                <span className="text-onair-mint">{Math.round(ttsProgress)}%</span>
              </div>
              <Progress value={ttsProgress} className="h-2" />
            </div>
          )}
        </div>

        {/* ë…¹ìŒ ì»¨íŠ¸ë¡¤ëŸ¬ ì¶”ê°€ */}
        <div className="mt-4">
          <div className="text-center space-y-4">
            <h3 className="text-lg font-semibold text-onair-text">
              {isRecording ? "ë…¹ìŒ ì¤‘..." : hasRecorded ? "ë…¹ìŒ ì™„ë£Œ!" : " "}
            </h3>

            {isRecording && (
              <>
                <div className="flex items-center justify-center space-x-1 h-16">
                  {isClient && waveformHeights.length > 0 ? (
                    waveformHeights.map((height, i) => (
                      <div
                        key={i}
                        className="bg-onair-orange rounded-full animate-wave"
                        style={{
                          width: "4px",
                          height: `${height}px`,
                          animationDelay: `${i * 0.1}s`,
                        }}
                      />
                    ))
                  ) : (
                    Array.from({ length: 20 }).map((_, i) => (
                      <div
                        key={i}
                        className="bg-onair-orange rounded-full animate-wave"
                        style={{
                          width: "4px",
                          height: "30px",
                          animationDelay: `${i * 0.1}s`,
                        }}
                      />
                    ))
                  )}
                </div>
                <p className="text-onair-text-sub text-sm mt-2">
                  {` ${Math.floor(recordingTime / 60)}:${(recordingTime % 60).toString().padStart(2, '0')}`}
                </p>
              </>
            )}

            <div className="flex flex-col items-center gap-2">
              {hasRecorded && !isRecording && audioURL && (
                // hidden ì²˜ë¦¬ (ìˆ¨ê¹€ ì²˜ë¦¬)
                <div className="w-full mb-4 hidden">
                  <WaveformPlayer 
                    ref={waveformRef} 
                    url={audioURL} 
                    onPlayStateChange={setIsPlaying}
                  />
                </div>
              )}
              <Button
                onClick={handleRecord}
                size="lg"
                className={`${isRecording
                  ? "bg-red-500 hover:bg-red-600 text-white"
                  : "bg-onair-mint hover:bg-onair-mint/90 text-onair-bg"
                  } font-semibold`}
              >
                {isRecording ? (
                  <>
                    <Square className="w-5 h-5 mr-2" />
                    ë…¹ìŒ ì¤‘ì§€
                  </>
                ) : (
                  <>
                    <Mic className="w-5 h-5 mr-2" />
                    {hasRecorded ? "ë‹¤ì‹œ ë…¹ìŒ" : "ë…¹ìŒ ì‹œì‘"}
                  </>
                )}
              </Button>

              {hasRecorded && !isRecording && audioURL && (
                <div className="flex gap-2">
                  <Button
                    onClick={handlePlay}
                    size="lg"
                    variant="outline"
                    className="border-onair-blue text-onair-blue hover:bg-onair-blue hover:text-onair-bg"
                  >
                    {isPlaying ? <Pause className="w-5 h-5 mr-2" /> : <Play className="w-5 h-5 mr-2" />}
                    {isPlaying ? "ì¼ì‹œì •ì§€" : "ì¬ìƒ"}
                  </Button>
                  <Button
                    onClick={handleEvaluate}
                    size="lg"
                    variant="outline"
                    className="border-onair-mint text-onair-mint hover:bg-onair-mint hover:text-onair-bg"
                    disabled={isAnalyzing || !uploadedRecordingUrl || hasAnalyzed}
                  >
                    {isAnalyzing ? (
                      <>
                        <RefreshCw className="w-5 h-5 mr-2 animate-spin" />
                        í‰ê°€ ì¤‘...
                      </>
                    ) : hasAnalyzed ? (
                      <>
                        <MessageSquare className="w-5 h-5 mr-2" />
                        í‰ê°€ ì™„ë£Œ
                      </>
                    ) : (
                      <>
                        <MessageSquare className="w-5 h-5 mr-2" />
                        í‰ê°€í•˜ê¸°
                      </>
                    )}
                  </Button>
                  {/* <Button
                    onClick={handleDownload}
                    size="lg"
                    variant="outline"
                    className="border-onair-blue text-onair-blue hover:bg-onair-blue hover:text-onair-bg"
                  >
                    <Download className="w-5 h-5 mr-2" />
                    ë‹¤ìš´ë¡œë“œ
                  </Button> */}
                </div>
              )}
            </div>

            {hasRecorded && !isRecording && isAnalyzing && <LoadingMessage />}
          </div>
        </div>

        {/* ìˆ¨ê²¨ì§„ audio ìš”ì†Œ ì¶”ê°€ */}
        <audio 
          ref={audioRef}
          onEnded={() => setIsPlaying(false)}
          onError={(e) => {
            console.error('ì˜¤ë””ì˜¤ ì¬ìƒ ì˜¤ë¥˜:', e);
            setIsPlaying(false);
          }}
          style={{ display: 'none' }}
        />
      </CardContent>
    </Card>
  )
}